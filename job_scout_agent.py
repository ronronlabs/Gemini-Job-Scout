# -*- coding: utf-8 -*-
"""Job Scout Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OUlM04LgDbKnKGsKyTnEU0intlmG6rHd
"""

import os
import smtplib
import ssl
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import time
from datetime import datetime, timedelta

import google.generativeai as genai
import pandas as pd
from jobspy import scrape_jobs

# --- Configuration (Loaded from Environment Variables in Cloud Functions) ---
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
SENDER_EMAIL = os.environ.get("SENDER_EMAIL")
RECEIVER_EMAIL = os.environ.get("RECEIVER_EMAIL")
GMAIL_APP_PASSWORD = os.environ.get("GMAIL_APP_PASSWORD")

# --- User Search Parameters ---
KEYWORDS_LIST = ["Data Analytics", "Business Intelligence", "Data Science", "AI", "Digital Transformation"]
SEARCH_SITES = ["linkedin", "indeed"]
LOCATIONS_TO_SEARCH = ["Toronto", "Canada"]
RESULTS_TO_FETCH = 100
SCORE_THRESHOLD = 7

def run_job_search(request):
    """
    Main entry point for the Google Cloud Function.
    This function orchestrates the entire job search and email process.
    """
    print("üöÄ Starting job search agent...")

    # 1. Load Resume from local file
    try:
        with open('resume.txt', 'r', encoding='utf-8') as f:
            my_resume = f.read()
        print("‚úÖ Successfully loaded resume from resume.txt.")
    except FileNotFoundError:
        print("‚ùå CRITICAL ERROR: 'resume.txt' not found.")
        return "Error: Resume file not found.", 500

    # 2. Configure the AI Model
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        print("‚úÖ AI Model configured.")
    except Exception as e:
        print(f"‚ùå Configuration Error: {e}")
        return "Error: AI configuration failed.", 500

    # 3. Scrape Jobs
    all_jobs_dfs = []
    for location in LOCATIONS_TO_SEARCH:
        for keyword in KEYWORDS_LIST:
            print(f"   -> Scraping for '{keyword}' in '{location}'...")
            try:
                jobs_df_single = scrape_jobs(
                    site_name=SEARCH_SITES, search_term=keyword, location=location, results_wanted=RESULTS_TO_FETCH
                )
                if not jobs_df_single.empty:
                    all_jobs_dfs.append(jobs_df_single)
                print(f"      Found {len(jobs_df_single)} jobs.")
            except Exception as e:
                print(f"      ‚ùå Scraping Error for '{keyword}' in '{location}': {e}")
            time.sleep(2)

    if not all_jobs_dfs:
        print("üèÅ No jobs found across all searches. Exiting.")
        return "No jobs found.", 200

    jobs_df = pd.concat(all_jobs_dfs, ignore_index=True)
    jobs_df.drop_duplicates(subset=['job_url'], keep='first', inplace=True)
    print(f"‚úÖ Combined and deduplicated to {len(jobs_df)} unique jobs.")

    # 4. Apply Filters
    jobs_df = apply_filters(jobs_df)

    # 5. Perform AI Analysis
    good_matches = analyze_jobs(jobs_df, my_resume, model)

    # 6. Send the Email
    if good_matches:
        send_email_report(good_matches)
    else:
        print("üòî No high-scoring matches found after analysis. No email will be sent.")

    print("üèÅ Job search agent finished successfully.")
    return "Function executed successfully.", 200


def apply_filters(jobs_df):
    """Applies all the data cleaning and filtering steps."""
    print("\nüìç Applying Location Filter...")
    is_toronto = jobs_df['location'].str.contains('Toronto', case=False, na=False)
    is_empty_location = jobs_df['location'].isna() | (jobs_df['location'] == "")
    jobs_df = jobs_df[is_montreal | is_empty_location]
    print(f"‚úÖ Retained {len(jobs_df)} jobs (Toronto or Empty/Remote Location).")

    print("\nüìÖ Applying Date Filter...")
    jobs_df['date_posted_dt'] = pd.to_datetime(jobs_df['date_posted'], errors='coerce')
    two_week_ago = datetime.now() - timedelta(days=14)
    jobs_df = jobs_df[(jobs_df['date_posted_dt'] >= two_week_ago) | (jobs_df['date_posted_dt'].isna())]
    print(f"‚úÖ Retained {len(jobs_df)} jobs posted recently.")

    print("\nüõ°Ô∏è Applying Pre-Filter Shield...")
    TOPIC_KEYWORDS = ['data', 'analytic', 'digital', 'science', 'financ', 'model', 'intelligence', 'AI']
    SENIORITY_KEYWORDS = ['senior', 'sr.', 'manager', 'director', 'lead', 'head', 'principal', 'vp']
    BLOCKING_KEYWORDS = ['real estate', 'broker', 'comptable', 'payable', 'assistant', 'sales', 'analyst']
    def is_relevant(title):
        title_lower = title.lower()
        has_topic = any(topic in title_lower for topic in TOPIC_KEYWORDS)
        has_seniority = any(seniority in title_lower for seniority in SENIORITY_KEYWORDS)
        if has_topic and has_seniority:
            return not any(blocking in title_lower for blocking in BLOCKING_KEYWORDS)
        return False
    jobs_df = jobs_df[jobs_df['title'].apply(is_relevant)]
    print(f"‚úÖ Retained {len(jobs_df)} relevant jobs after keyword shield.")

    return jobs_df

def analyze_jobs(jobs_df, my_resume, model):
    """Uses the LLM to score each job and returns a list of good matches."""
    print(f"\nüß† Starting AI analysis on {len(jobs_df)} filtered jobs...")
    good_matches = []
    for index, job in jobs_df.iterrows():
        job_details = f"Title: {job['title']}\nCompany: {job['company']}\nDesc: {job.get('description', 'N/A')}"
        print(f"   Analyzing Job #{index + 1}/{len(jobs_df)}: '{job['title']}'...")

        prompt = f"""Analyze the fit between the Candidate's Resume and the Job Description below. **Resume:** --- {my_resume} --- **Job Description:** --- {job_details} --- **Task:** Provide a "Fit Score" from 1 to 10 and a concise, one-sentence "Reasoning". Format as: Fit Score: [Score]\nReasoning: [Sentence]"""

        try:
            response = model.generate_content(prompt, request_options={'timeout': 120})
            lines = response.text.strip().split('\n')
            score = int(lines[0].replace("Fit Score:", "").strip())
            reasoning = lines[1].replace("Reasoning:", "").strip()

            if score >= SCORE_THRESHOLD:
                match = job.to_dict()
                match.update({'score': score, 'reasoning': reasoning})
                good_matches.append(match)
        except Exception as e:
            print(f"      ‚ùå Error parsing AI analysis: {e}\n")
        time.sleep(1)

    return sorted(good_matches, key=lambda x: x['score'], reverse=True)

def send_email_report(good_matches):
    """Formats and sends the final email report."""
    print(f"\nüìß Preparing to send email with {len(good_matches)} top opportunities...")

    message = MIMEMultipart("alternative")
    today_date = datetime.now().strftime("%B %d, %Y")
    message["Subject"] = f"Your Personal Job Digest - {today_date}"
    message["From"] = SENDER_EMAIL
    message["To"] = RECEIVER_EMAIL

    # Create the HTML body of the email
    html = "<html><body>"
    html += f"<h2>Top {len(good_matches)} Job Opportunities Found Today</h2>"

    for i, job in enumerate(good_matches):
        html += f"<hr>"
        html += f"<h3>{i+1}. {job['title']}</h3>"
        html += f"<p><b>Company:</b> {job['company']}<br>"
        html += f"<b>Location:</b> {job.get('location', 'N/A')}<br>"
        html += f"<b>‚≠ê Fit Score:</b> {job['score']}/10<br>"
        html += f"<b>üí¨ Reasoning:</b> <i>{job['reasoning']}</i></p>"
        html += f'<p><a href="{job["job_url"]}"><b>Apply Here</b></a></p>'

    html += "</body></html>"
    message.attach(MIMEText(html, "html"))

    # Send the email using Gmail's SMTP server
    context = ssl.create_default_context()
    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465, context=context) as server:
            server.login(SENDER_EMAIL, GMAIL_APP_PASSWORD)
            server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, message.as_string())
        print("‚úÖ Email sent successfully!")
    except Exception as e:
        print(f"‚ùå Failed to send email: {e}")